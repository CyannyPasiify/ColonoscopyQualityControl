# lightning.pytorch==2.0.1.post0
# 100_train_CleansingClassifier_ResNet101
seed_everything: 0
trainer:
  accelerator: gpu
  strategy: ddp
  devices: [ 3 ]
  num_nodes: 1
  precision: 32-true
  logger:
    - class_path: lightning.pytorch.loggers.TensorBoardLogger # note: enable logging to tensorboard, support image logging
      init_args:
        save_dir: "Experiment"            # note: directory to save log files
        name: "100_train_CleansingClassifier_ResNet101"          # note: name of the current experiment
        version: "tensorboard_train_val"  # note: version name specified by the logger, for formally experiments, please specify a version number
    - class_path: lightning.pytorch.loggers.CSVLogger # note: enable logging to csv, doesn't support image logging
      init_args:
        save_dir: "Experiment"            # note: identical to other loggers' is ok, they'll save log files to the same directory
        name: "100_train_CleansingClassifier_ResNet101"          # note: keep the same experiment name
        version: "csv_train_val"          # note: version name specified by the logger, for formally experiments, please specify a version number
  callbacks:
    - class_path: lightning.pytorch.callbacks.progress.TQDMProgressBar # note: display a tqdm progress bar
      init_args:
        refresh_rate: 20 # note: after processing these batches, tqdm'll update its display
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint # note: specify ModelCheckpoint behavior
      init_args:
        save_last: true # note: save the last.ckpt which contains the latest net weights and optimizer parameters
        monitor: "epoch"
        mode: max
        every_n_epochs: 1 # note: save .ckpt every n epochs
        filename: "CleansingClassifier_{epoch:03d}" # note: inject epoch into .ckpt filename, make sure you had
        save_top_k: 5
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        monitor: "val_acc" # note: save .ckpt with the highest val_acc
        mode: max
        filename: "CleansingClassifier_best_val_acc_{epoch:03d}_{val_acc:.4f}" # note: inject val_metric_l1_err & val_metric_l2_err into .ckpt filename, make sure you had logged these two metrics in validation_step()
  fast_dev_run: false
  max_epochs: 500
  min_epochs: null
  max_steps: -1
  min_steps: null
  max_time: null
  limit_train_batches: null
  limit_val_batches: null
  limit_test_batches: null
  limit_predict_batches: null
  overfit_batches: 0.0
  val_check_interval: null
  check_val_every_n_epoch: 1
  num_sanity_val_steps: null
  log_every_n_steps: 10
  enable_checkpointing: true
  enable_progress_bar: null
  enable_model_summary: null
  accumulate_grad_batches: 1
  gradient_clip_val: null
  gradient_clip_algorithm: null
  deterministic: null
  benchmark: null
  inference_mode: true
  use_distributed_sampler: true
  profiler: null
  detect_anomaly: false
  barebones: false
  plugins: null
  sync_batchnorm: false
  reload_dataloaders_every_n_epochs: 0
  default_root_dir: "Experiment/100_train_CleansingClassifier_ResNet101"
ckpt_path: null # note: reload from checkpoint
data:
  class_path: Classifier.MultiClassificationDataModule
  init_args:
    dataset_root: "/mnt/data/cwy/Datasets/UIHNJMuLv3"
    index_file_path: "/mnt/data/cwy/Datasets/UIHNJMuLv3/cls_folds/job_cleansing_train_validation_test_fold.json"
    resize_shape:
      - 336
      - 336
    center_crop_shape:
      - 336
      - 336
    brightness_jitter: 0.8
    contrast_jitter: 0.8
    saturation_jitter: 0.8
    batch_size: 64
    num_workers: 8
model:
  class_path: Classifier.CleansingClassifier
  init_args:
    input_shape:
      - 336
      - 336
    num_classes: 4
    batch_size: 64
    lr: 0.0001
    b1: 0.9
    b2: 0.999
    epochs: 500