# lightning.pytorch==2.0.1.post0
# nohup python DeployLauncher.py fit --config Config/multi_label_classify_config/R102_finetuneR101_400.yaml > ./log/R102_finetuneR101_400.log &
# R102_finetuneR101_400
seed_everything: 0
trainer:
  accelerator: gpu
  strategy: ddp
  devices: [ 5, 6, 7, 8 ]
  num_nodes: 1
  precision: 32-true
  logger:
    - class_path: lightning.pytorch.loggers.TensorBoardLogger # note: enable logging to tensorboard, support image logging
      init_args:
        save_dir: "Experiment"            # note: directory to save log files
        name: "R102_finetuneR101_400"          # note: name of the current experiment
        version: "tensorboard_finetune"  # note: version name specified by the logger, for formally experiments, please specify a version number
    - class_path: lightning.pytorch.loggers.CSVLogger # note: enable logging to csv, doesn't support image logging
      init_args:
        save_dir: "Experiment"            # note: identical to other loggers' is ok, they'll save log files to the same directory
        name: "R102_finetuneR101_400"          # note: keep the same experiment name
        version: "csv_finetune"          # note: version name specified by the logger, for formally experiments, please specify a version number
  callbacks:
    - class_path: lightning.pytorch.callbacks.progress.TQDMProgressBar # note: display a tqdm progress bar
      init_args:
        refresh_rate: 20 # note: after processing these batches, tqdm'll update its display
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint # note: specify ModelCheckpoint behavior
      init_args:
        save_last: true # note: save the last.ckpt which contains the latest net weights and optimizer parameters
        monitor: "epoch"
        mode: max
        every_n_epochs: 50 # note: save .ckpt every n epochs
        filename: "MuLModel_{epoch:03d}" # note: inject epoch into .ckpt filename, make sure you had
        save_top_k: 8
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        monitor: "val_thresh_mean_acc" # note: save .ckpt with the highest val_thresh_mean_acc
        mode: max
        filename: "MuLModel_best_mAcc_{epoch:03d}_{val_thresh_mean_acc:.4f}"
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        monitor: "label_outside_acc" # note: save .ckpt with the highest label_outside_acc
        mode: max
        filename: "MuLModel_best_ioAcc_{epoch:03d}_{label_outside_acc:.4f}"
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        monitor: "label_nonsense_acc" # note: save .ckpt with the highest label_nonsense_acc
        mode: max
        filename: "MuLModel_best_nsAcc_{epoch:03d}_{label_nonsense_acc:.4f}"
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        monitor: "label_ileocecal_acc_thresh" # note: save .ckpt with the highest label_ileocecal_acc
        mode: max
        filename: "MuLModel_best_ileoAcc_{epoch:03d}_{label_ileocecal_acc_thresh:.4f}"
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        monitor: "label_ileocecal_prec_thresh" # note: save .ckpt with the highest label_ileocecal_prec
        mode: max
        filename: "MuLModel_best_ileoPrec_{epoch:03d}_{label_ileocecal_prec_thresh:.4f}"
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        monitor: "label_cleansing_acc_thresh" # note: save .ckpt with the highest label_cleansing_acc
        mode: max
        filename: "MuLModel_best_cls4Acc_{epoch:03d}_{label_cleansing_acc_thresh:.4f}"
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        monitor: "label_cleansing_biclassify_acc_thresh" # note: save .ckpt with the highest label_cleansing_biclassify_acc
        mode: max
        filename: "MuLModel_best_cls2Acc_{epoch:03d}_{label_cleansing_biclassify_acc_thresh:.4f}"
  fast_dev_run: false
  max_epochs: 400
  min_epochs: null
  max_steps: -1
  min_steps: null
  max_time: null
  limit_train_batches: null
  limit_val_batches: null
  limit_test_batches: null
  limit_predict_batches: null
  overfit_batches: 0.0
  val_check_interval: null
  check_val_every_n_epoch: 1
  num_sanity_val_steps: null
  log_every_n_steps: 10
  enable_checkpointing: true
  enable_progress_bar: null
  enable_model_summary: null
  accumulate_grad_batches: 1
  gradient_clip_val: null
  gradient_clip_algorithm: null
  deterministic: null
  benchmark: null
  inference_mode: true
  use_distributed_sampler: true
  profiler: null
  detect_anomaly: false
  barebones: false
  plugins: null
  sync_batchnorm: false
  reload_dataloaders_every_n_epochs: 0
  default_root_dir: "Experiment/R102_finetuneR101_400"
ckpt_path: "Experiment/R101_train_400/tensorboard_fit/checkpoints/last.ckpt" # note: reload from checkpoint
data:
  class_path: MultiLabelClassifier.ColonoscopyMultiLabelDataModule
  init_args:
    data_index_file: "../Datasets/UIHNJMuL/cls_folds/fold0.json"
    data_root: "../Datasets/UIHNJMuL"
    sample_weight:
      nobbps: 0
      bbps0: 0
      bbps1: 0
      bbps2: 10000
      bbps3: 10000
    resize_shape:
      - 224
      - 224
    center_crop_shape:
      - 224
      - 224
    brightness_jitter: 0.8
    contrast_jitter: 0.8
    saturation_jitter: 0.8
    batch_size: 16
    num_workers: 16
    dry_run: false
model:
  class_path: MultiLabelClassifier.MultiLabelClassifier_ViT_L_Patch16_224_Class7
  init_args:
    input_shape:
      - 224
      - 224
    num_heads: 8
    attention_lambda: 0.3
    num_classes: 7
    thresh: 0.5
    batch_size: 16
    lr: 0.00001
    epochs: 400
    momentum: 0.9
    weight_decay: 0.0001
    cls_weight: 10.0
    outside_acc_thresh: 0.9
    nonsense_acc_thresh: 0.9
    test_viz_save_dir: "Experiment/R102_finetuneR101_400/test_viz"