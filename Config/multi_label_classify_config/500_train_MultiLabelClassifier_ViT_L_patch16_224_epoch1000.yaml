# lightning.pytorch==2.0.1.post0
# 500_train_MultiLabelClassifier_ViT_L_patch16_224_epoch1000
seed_everything: 0
trainer:
  accelerator: gpu
  strategy: ddp
  devices: [ 3, 4, 5, 6 ]
  num_nodes: 1
  precision: 32-true
  logger:
    - class_path: lightning.pytorch.loggers.TensorBoardLogger # note: enable logging to tensorboard, support image logging
      init_args:
        save_dir: "Experiment"            # note: directory to save log files
        name: "500_train_MultiLabelClassifier_ViT_L_patch16_224_epoch1000"          # note: name of the current experiment
        version: "tensorboard_train_val"  # note: version name specified by the logger, for formally experiments, please specify a version number
    - class_path: lightning.pytorch.loggers.CSVLogger # note: enable logging to csv, doesn't support image logging
      init_args:
        save_dir: "Experiment"            # note: identical to other loggers' is ok, they'll save log files to the same directory
        name: "500_train_MultiLabelClassifier_ViT_L_patch16_224_epoch1000"          # note: keep the same experiment name
        version: "csv_train_val"          # note: version name specified by the logger, for formally experiments, please specify a version number
  callbacks:
    - class_path: lightning.pytorch.callbacks.progress.TQDMProgressBar # note: display a tqdm progress bar
      init_args:
        refresh_rate: 20 # note: after processing these batches, tqdm'll update its display
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint # note: specify ModelCheckpoint behavior
      init_args:
        save_last: true # note: save the last.ckpt which contains the latest net weights and optimizer parameters
        monitor: "epoch"
        mode: max
        every_n_epochs: 1 # note: save .ckpt every n epochs
        filename: "CleansingClassifier_{epoch:03d}" # note: inject epoch into .ckpt filename, make sure you had
        save_top_k: 3
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        monitor: "val_mean_acc" # note: save .ckpt with the highest val_acc
        mode: max
        filename: "CleansingClassifier_best_val_acc_{epoch:03d}_{val_mean_acc:.4f}" # note: inject val_metric_l1_err & val_metric_l2_err into .ckpt filename, make sure you had logged these two metrics in validation_step()
  fast_dev_run: false
  max_epochs: 1000
  min_epochs: null
  max_steps: -1
  min_steps: null
  max_time: null
  limit_train_batches: null
  limit_val_batches: null
  limit_test_batches: null
  limit_predict_batches: null
  overfit_batches: 0.0
  val_check_interval: null
  check_val_every_n_epoch: 1
  num_sanity_val_steps: null
  log_every_n_steps: 10
  enable_checkpointing: true
  enable_progress_bar: null
  enable_model_summary: null
  accumulate_grad_batches: 1
  gradient_clip_val: null
  gradient_clip_algorithm: null
  deterministic: null
  benchmark: null
  inference_mode: true
  use_distributed_sampler: true
  profiler: null
  detect_anomaly: false
  barebones: false
  plugins: null
  sync_batchnorm: false
  reload_dataloaders_every_n_epochs: 0
  default_root_dir: "Experiment/500_train_MultiLabelClassifier_ViT_L_patch16_224_epoch1000"
ckpt_path: null # note: reload from checkpoint
data:
  class_path: MultiLabelClassifier.ColonoscopyMultiLabelDataModule
  init_args:
    image_index_file_or_root: "/mnt/data4/cwy/Datasets/UIHMuL/folds/fold0.json"
    sample_weight:
      ileocecal: 4500
      nofeature: 4500
      outside: 4500
    resize_shape:
      - 268
      - 268
    center_crop_shape:
      - 224
      - 224
    brightness_jitter: 0.8
    contrast_jitter: 0.8
    saturation_jitter: 0.8
    batch_size: 16
    num_workers: 8
    dry_run: false
model:
  class_path: MultiLabelClassifier.MultiLabelClassifier_ViT_L_Patch16_224
  init_args:
    input_shape:
      - 224
      - 224
    num_heads: 8
    attention_lambda: 0.3
    num_classes: 6
    thresh: 0.5
    batch_size: 16
    lr: 0.0001
    momentum: 0.9
    weight_decay: 0.0001
    step_size: 5